# 1.1，1.2求解代码

```python
import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
import seaborn as sns  
from sklearn.ensemble import RandomForestRegressor  
from sklearn.model_selection import train_test_split, GridSearchCV  
from sklearn.metrics import mean_squared_error, r2_score  
from sklearn.preprocessing import StandardScaler


# 加载四个CSV文件  
medal_counts = pd.read_csv('D:/meisai-2025C/2025_Problem_C_Data/summerOly_medal_counts.csv', encoding='ISO-8859-1')  
programs = pd.read_csv('D:/meisai-2025C/2025_Problem_C_Data/summerOly_programs.csv', encoding='ISO-8859-1')  
athletes = pd.read_csv('D:/meisai-2025C/2025_Problem_C_Data/summerOly_athletes.csv', encoding='ISO-8859-1')  
hosts = pd.read_csv('D:/meisai-2025C/2025_Problem_C_Data/summerOly_hosts.csv', encoding='ISO-8859-1')

#数据清洗和预处理
# 修复hosts数据框的列名  
hosts.columns = hosts.columns.str.strip()  
hosts.rename(columns={hosts.columns[0]: 'Year'}, inplace=True)  

# 转换相关列为数值类型  
medal_counts['Year'] = pd.to_numeric(medal_counts['Year'], errors='coerce')  
programs_years = programs.loc[:, programs.columns.str.isnumeric()]  # 提取年份列

#合并数据集
# 合并medal_counts和hosts数据  
medal_counts = medal_counts.merge(hosts, left_on='Year', right_on='Year', how='left')  
medal_counts['is_host'] = (medal_counts['NOC'] == medal_counts['Host']).astype(int)

#特征提取
# 提取建模所需的特征  
features = ['Year', 'Gold', 'Silver', 'Bronze', 'Total', 'is_host', 'NOC']  
data = medal_counts[features]  
data = data.dropna()

#特征工程
# 添加人口和GDP特征（示例数据）  
data['population'] = np.random.randint(1e6, int(1e8), size=len(data))  
data['gdp'] = np.random.randint(1e9, 1e12, size=len(data), dtype=np.int64)

# Adding lag features (previous year's Gold and Total medals)  
data['Gold_lag'] = data.groupby('NOC')['Gold'].shift(1).fillna(0)  
data['Total_lag'] = data.groupby('NOC')['Total'].shift(1).fillna(0)  

# Update feature set  
X = data[['Year', 'population', 'gdp', 'is_host', 'Gold_lag', 'Total_lag']]  
y_gold = data['Gold']  

# Step 3: Random Forest Regression with Hyperparameter Tuning  
# Train-test split  
X_train, X_test, y_train_gold, y_test_gold = train_test_split(X, y_gold, test_size=0.2, random_state=42)  

# Hyperparameter tuning using GridSearchCV  
param_grid = {  
    'n_estimators': [100, 200, 300],  
    'max_depth': [None, 10, 20],  
    'min_samples_split': [2, 5, 10],  
    'min_samples_leaf': [1, 2, 4]  
}  

rf_model = RandomForestRegressor(random_state=42)  
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring="neg_mean_squared_error", verbose=2, n_jobs=-1)  
grid_search.fit(X_train, y_train_gold)  

# Best parameters from grid search  
best_rf_model = grid_search.best_estimator_  
print("Best Parameters:", grid_search.best_params_)  

# Evaluate the optimized model  
gold_pred_rf = best_rf_model.predict(X_test)  
gold_mse_rf = mean_squared_error(y_test_gold, gold_pred_rf)  
gold_r2_rf = r2_score(y_test_gold, gold_pred_rf)  

print(f"Gold Medal Optimized Random Forest Model MSE: {gold_mse_rf}")  
print(f"Gold Medal Optimized Random Forest R2 Score: {gold_r2_rf}")  

# Feature importance  
feature_importances = pd.Series(best_rf_model.feature_importances_, index=X.columns)  

# Step 4: Visualizing Model Results  
# Plot feature importance  
plt.figure(figsize=(10, 6))  
feature_importances.sort_values().plot(kind='barh', title='Feature Importance (Optimized Random Forest)', color='skyblue')  
plt.xlabel('Importance')  
plt.ylabel('Features')  
plt.show()
# Plot actual vs predicted values  
plt.figure(figsize=(10, 6))  
sns.scatterplot(x=y_test_gold, y=gold_pred_rf, color='orange', edgecolor='b', s=80)  
plt.plot([y_test_gold.min(), y_test_gold.max()], [y_test_gold.min(), y_test_gold.max()], '-r', linewidth=2)  
plt.title('Actual vs Predicted Gold Medals')  
plt.xlabel('Actual Gold Medals')  
plt.ylabel('Predicted Gold Medals')  
plt.show()  

# Plot residuals  
residuals = y_test_gold - gold_pred_rf  
plt.figure(figsize=(10, 6))  
sns.histplot(residuals, kde=True, color='purple', bins=20)  
plt.title('Residuals Distribution')  
plt.xlabel('Residuals')  
plt.ylabel('Frequency')  
plt.show()  

# Step 5: Predicting for 2028  
# Example input for 2028 prediction (modify based on actual feature values)  
example_2028 = pd.DataFrame({  
    'Year': [2028],  
    'population': [350000000],  # Example value  
    'gdp': [1.5e12],           # Example value  
    'is_host': [1],            # USA hosting in 2028  
    'Gold_lag': [40],          # Example: previous gold count  
    'Total_lag': [120]         # Example: previous total medals  
})  

# Predict 2028 Gold Medals  
gold_pred_2028 = best_rf_model.predict(example_2028)  
print(f"Predicted Gold Medals for 2028: {gold_pred_2028[0]}")  

# Visualize feature importance for prediction  
plt.figure(figsize=(10, 6))  
feature_importances.sort_values().plot(kind='barh', title='Feature Importance for 2028 Prediction', color='lightgreen')  
plt.xlabel('Importance')  
plt.ylabel('Features')  
plt.show()

```

# 1.3问求解代码

```python
import pandas as pd  
import numpy as np  
from sklearn.model_selection import train_test_split  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, auc
from imblearn.over_sampling import SMOTE  
import matplotlib.pyplot as plt  
import seaborn as sns  

# Step 1: Load and preprocess data  
medal_counts = pd.read_csv('D:/meisai-2025C/2025_Problem_C_Data/summerOly_medal_counts.csv', encoding="ISO-8859-1")  

# Add population, GDP, and lag features to the dataset  
np.random.seed(42)  
medal_counts['population'] = np.random.randint(1e6, 1e7, size=len(medal_counts))  
medal_counts['gdp'] = np.random.uniform(1e9, 1e12, size=len(medal_counts)).astype(int)  
medal_counts['Gold_lag'] = medal_counts.groupby('NOC')['Gold'].shift(1).fillna(0)  
medal_counts['Total_lag'] = medal_counts.groupby('NOC')['Total'].shift(1).fillna(0)  

# Define the target variable: first_medal (whether a country has won at least one medal)  
medal_counts['first_medal'] = (medal_counts['Total'] > 0).astype(int)  

# Step 2: Create synthetic data for countries with no medals to balance the dataset  
synthetic_data = pd.DataFrame({  
    'NOC': [f'SYN{i}' for i in range(200)],  
    'Gold': [0] * 200,  
    'Silver': [0] * 200,  
    'Bronze': [0] * 200,  
    'Total': [0] * 200,  
    'Year': [2028] * 200,  
    'population': np.random.randint(1e6, 1e7, 200),  
    'gdp': np.random.uniform(1e9, 1e12, size=200).astype(int),  
    'Gold_lag': [0] * 200,  
    'Total_lag': [0] * 200,  
    'first_medal': [0] * 200  
})  

# Combine original and synthetic data  
combined_data = pd.concat([medal_counts, synthetic_data], ignore_index=True)  

# Step 3: Feature engineering: Log-transform GDP to normalize the distribution  
combined_data['gdp'] = combined_data['gdp'].replace(0, 1)  # Replace zero GDP with 1  
combined_data['log_gdp'] = np.log1p(combined_data['gdp'])  # Log-transform GDP  
combined_data = combined_data.fillna(0)  # Replace any NaN values with 0  

# Define features and target  
features = ['population', 'log_gdp', 'Gold_lag', 'Total_lag']  
X = combined_data[features]  
y = combined_data['first_medal']  

# Step 4: Address class imbalance using SMOTE  
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)  

# Step 5: Train-test split  
X_train, X_test, y_train, y_test = train_test_split(  
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled  
)  

# Step 6: Train Random Forest Classifier with balanced class weights  
rf_clf = RandomForestClassifier(random_state=42, n_estimators=300, max_depth=20, class_weight='balanced')  
rf_clf.fit(X_train, y_train)  
y_pred = rf_clf.predict(X_test)  
y_pred_prob = rf_clf.predict_proba(X_test)[:, 1]  

# Step 7: Evaluate the model  
accuracy = accuracy_score(y_test, y_pred)  
roc_auc = roc_auc_score(y_test, y_pred_prob)  
classification_report_output = classification_report(y_test, y_pred)  

# Step 8: Visualize feature importance  
feature_importances = pd.DataFrame({  
    'Feature': features,  
    'Importance': rf_clf.feature_importances_  
}).sort_values(by='Importance', ascending=False)  

plt.figure(figsize=(8, 6))  
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')  
plt.title('Feature Importance for First Medal Prediction', fontsize=14)  
plt.xlabel('Importance', fontsize=12)  
plt.ylabel('Feature', fontsize=12)  
plt.tight_layout()  
plt.show()  

# Step 9: Visualize ROC Curve  
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)  
roc_auc_value = auc(fpr, tpr)  

plt.figure(figsize=(8, 6))  
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')  
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  
plt.xlabel('False Positive Rate', fontsize=12)  
plt.ylabel('True Positive Rate', fontsize=12)  
plt.title('ROC Curve for First Medal Prediction', fontsize=14)  
plt.legend(loc='lower right', fontsize=10)  
plt.tight_layout()  
plt.show()  

# Display results  
print("Accuracy:", accuracy)
print("ROC AUC:", roc_auc)  
print("Classification Report:\n", classification_report_output)  
print("Feature Importances:\n", feature_importances)
```

# 1.4求解代码

```python
import pandas as pd  
import numpy as np  
from sklearn.model_selection import train_test_split, GridSearchCV  
from sklearn.linear_model import LinearRegression  
from sklearn.ensemble import RandomForestRegressor  
from sklearn.metrics import mean_squared_error, r2_score  
from xgboost import XGBRegressor  
import matplotlib.pyplot as plt  
import seaborn as sns  

# File paths  
medal_counts_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_medal_counts.csv'  
hosts_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_hosts.csv'  
programs_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_programs.csv'  

# Load data  
medal_counts = pd.read_csv(medal_counts_path, encoding="ISO-8859-1")  
hosts = pd.read_csv(hosts_path, encoding="utf-8-sig")  
programs = pd.read_csv(programs_path, encoding="ISO-8859-1")  

# Standardize column names  
hosts.columns = hosts.columns.str.strip()  
medal_counts.columns = medal_counts.columns.str.strip()  
programs.columns = programs.columns.str.strip()  

# Process programs file  
program_years = programs.columns[5:]  # Assuming the first 5 columns are metadata  

# Reshape the programs data to long format  
programs_long = programs.melt(id_vars=['Sport', 'Discipline'], value_vars=program_years,  
                              var_name='Year', value_name='Event_Count')  

# Clean the 'Year' column to remove invalid characters  
programs_long['Year'] = programs_long['Year'].str.extract(r'(\d+)').astype(int)  

# Replace non-numeric values in 'Event_Count' with 0 and convert to integer  
programs_long['Event_Count'] = programs_long['Event_Count'].replace({'?': 0}).astype(str)  
programs_long['Event_Count'] = programs_long['Event_Count'].str.extract(r'(\d+)').fillna(0).astype(int)  

# Group by Year to get total Event Count  
event_counts = programs_long.groupby('Year')['Event_Count'].sum().reset_index()
# Merge with medal_counts  
if 'Year' in hosts.columns and 'Year' in medal_counts.columns:  
    medal_counts = medal_counts.merge(hosts, on='Year', how='left')  
else:  
    print("Hosts columns:", hosts.columns)  
    print("Medal counts columns:", medal_counts.columns)  
    raise KeyError("'Year' column missing in either medal_counts or hosts dataframe.")  

# Add host country indicator  
medal_counts['is_host'] = (medal_counts['NOC'] == medal_counts['Host']).astype(int)  

# Merge event counts with medal_counts  
medal_counts = medal_counts.merge(event_counts, on='Year', how='left')  

# Check and add 'population' column if missing  
if 'population' not in medal_counts.columns:  
    np.random.seed(42)  
    medal_counts['population'] = np.random.randint(1e6, 1e7, size=len(medal_counts))  
    print("'population' column added.")  

# Check and add 'gdp' column if missing  
if 'gdp' not in medal_counts.columns:  
    medal_counts['gdp'] = np.random.uniform(1e9, 1e12, size=len(medal_counts)).astype(int)  
    print("'gdp' column added.")  

# Add lag features for medals  
medal_counts['Gold_lag'] = medal_counts.groupby('NOC')['Gold'].shift(1).fillna(0)  
medal_counts['Total_lag'] = medal_counts.groupby('NOC')['Total'].shift(1).fillna(0)  

# Feature selection  
features = ['Event_Count', 'is_host', 'Gold_lag', 'Total_lag', 'population']  
X = medal_counts[features].copy()  
y = medal_counts['Total']  

# Check for NaN or infinite values in features and target  
X.replace([np.inf, -np.inf], np.nan, inplace=True)  
y.replace([np.inf, -np.inf], np.nan, inplace=True)  

# Drop rows with NaN values in features or target  
X = X.dropna()  
y = y[X.index]  # Ensure the target corresponds to the cleaned features  

# Train-test split  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  

# Linear Regression Model  
lr_model = LinearRegression()
以下是从图片中提取的文本内容：

```python
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Linear Regression Evaluation
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

# Random Forest Model with Hyperparameter Tuning
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
rf_model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=3, 
                           scoring='r2', verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best Random Forest Model
best_rf_model = grid_search.best_estimator_
y_pred_rf = best_rf_model.predict(X_test)

# Random Forest Evaluation
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# XGBoost Model with Hyperparameter Tuning
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0]
}
xgb_model = XGBRegressor(random_state=42, eval_metric='rmse')
grid_search_xgb = GridSearchCV(estimator=xgb_model, 
                               param_grid=xgb_param_grid, cv=3, scoring='r2', 
                               verbose=1, n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Best XGBoost Model
best_xgb_model = grid_search_xgb.best_estimator_
y_pred_xgb = best_xgb_model.predict(X_test)

# XGBoost Evaluation
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

# Feature Importance (XGBoost)
xgb_feature_importances = pd.DataFrame({
    'Feature': features,
'Importance': best_xgb_model.feature_importances_  
}).sort_values(by='Importance', ascending=False)  

# Visualizations  
# Feature Importance  
plt.figure(figsize=(8, 6))  
sns.barplot(x='Importance', y='Feature', data=xgb_feature_importances, palette='viridis')  
plt.title('Feature Importance for Medal Prediction (XGBoost)', fontsize=14)  
plt.xlabel('Importance', fontsize=12)  
plt.ylabel('Feature', fontsize=12)  
plt.tight_layout()  
plt.show()  

# Linear Regression Results  
plt.figure(figsize=(8, 6))  
plt.scatter(y_test, y_pred_lr, alpha=0.7, label='Linear Regression')  
plt.plot([0, max(y_test)], [0, max(y_test)], color='red', linestyle='--', label='Ideal Fit')  
plt.title('Linear Regression: Actual vs Predicted', fontsize=14)  
plt.xlabel('Actual Total Medals', fontsize=12)  
plt.ylabel('Predicted Total Medals', fontsize=12)  
plt.legend()  
plt.tight_layout()  
plt.show()  

# Random Forest Results  
plt.figure(figsize=(8, 6))  
plt.scatter(y_test, y_pred_rf, alpha=0.7, label='Optimized Random Forest')  
plt.plot([0, max(y_test)], [0, max(y_test)], color='red', linestyle='--', label='Ideal Fit')  
plt.title('Optimized Random Forest: Actual vs Predicted', fontsize=14)  
plt.xlabel('Actual Total Medals', fontsize=12)  
plt.ylabel('Predicted Total Medals', fontsize=12)  
plt.legend()  
plt.tight_layout()  
plt.show()  

# XGBoost Results  
plt.figure(figsize=(8, 6))  
plt.scatter(y_test, y_pred_xgb, alpha=0.7, label='Optimized XGBoost')  
plt.plot([0, max(y_test)], [0, max(y_test)], color='red', linestyle='--', label='Ideal Fit')  
plt.title('Optimized XGBoost: Actual vs Predicted', fontsize=14)  
plt.xlabel('Actual Total Medals', fontsize=12)  
plt.ylabel('Predicted Total Medals', fontsize=12)  
plt.legend()  
plt.tight_layout()  
plt.show()  

# Display results  
print("Linear Regression MSE:", mse_lr)  
print("Linear Regression R2:", r2_lr)
print("Optimized Random Forest MSE:", mse_rf)  
print("Optimized Random Forest R2:", r2_rf)  
print("Optimized XGBoost MSE:", mse_xgb)  
print("Optimized XGBoost R2:", r2_xgb)  
print("\nFeature Importances (Optimized XGBoost):\n", xgb_feature_importances)
```

# 2.1

```python
import pandas as pd  
import numpy as np  
from sklearn.ensemble import RandomForestRegressor  
from sklearn.model_selection import train_test_split, GridSearchCV  
from sklearn.metrics import mean_squared_error, r2_score  
import matplotlib.pyplot as plt  
import seaborn as sns  

# 文件路径  
hosts_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_hosts.csv'  
medal_counts_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_medal_counts.csv'  

# 数据加载  
hosts = pd.read_csv(hosts_path, encoding='utf-8-sig')  
medal_counts = pd.read_csv(medal_counts_path, encoding="ISO-8859-1")  

# 修复列名中的特殊字符  
print("Hosts columns before rename:", hosts.columns)  
hosts.columns = hosts.columns.str.strip().str.replace('[^\w]', '', regex=True)  # 移除特殊字符  
medal_counts.columns = medal_counts.columns.str.strip()  

# 确保 'Year' 列存在且为整数类型  
if 'Year' in hosts.columns:  
    hosts['Year'] = hosts['Year'].astype(str).str.extract(r'(\d+)').astype(int)  # 提取年份数字并转换为整数  
else:  
    raise KeyError("Column 'Year' not found in hosts dataset. Please verify the dataset.")  

if 'Year' in medal_counts.columns:  
    medal_counts['Year'] = medal_counts['Year'].astype(int)  
else:  
    raise KeyError("Column 'Year' not found in medal_counts dataset. Please verify the dataset.")  

# 合并数据  
data = pd.merge(medal_counts, hosts, on='Year', how='left')  

# 检查合并后的数据  
print("Merged Data Sample:\n", data.head())
# 模型部分  
# 假设我们想预测奖牌总数（Total）与其他特征之间的关系  

# 特征选择  
features = ['Year', 'Gold', 'Silver', 'Bronze']  
target = 'Total'  

X = data[features].dropna()  # 删除含有 NaN 的行  
y = data[target].dropna()  

# 划分数据集  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  

# 随机森林模型  
rf_model = RandomForestRegressor(random_state=42)  
param_grid = {  
    'n_estimators': [50, 100, 200],  
    'max_depth': [5, 10, None],  
    'min_samples_split': [2, 5, 10]  
}  

# 网格搜索  
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)  
grid_search.fit(X_train, y_train)  

# 最优模型  
best_rf_model = grid_search.best_estimator_  

# 评估  
y_pred = best_rf_model.predict(X_test)  
mse = mean_squared_error(y_test, y_pred)  
r2 = r2_score(y_test, y_pred)  

print(f"Optimized Random Forest MSE: {mse}")  
print(f"Optimized Random Forest R2: {r2}")  

# 可视化特征重要性  
feature_importances = pd.DataFrame({  
    'Feature': features,  
    'Importance': best_rf_model.feature_importances_  
}).sort_values(by='Importance', ascending=False)  

plt.figure(figsize=(8, 6))  
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')  
plt.title('Feature Importance for Medal Prediction', fontsize=14)  
plt.xlabel('Importance', fontsize=12)  
plt.ylabel('Feature', fontsize=12)  

plt.tight_layout()  
plt.show()  

# 显示结果  
print("Feature Importances:\n", feature_importances)
```

# 2.2

```python
import pandas as pd  
import numpy as np  
from sklearn.cluster import KMeans  
from sklearn.ensemble import RandomForestRegressor  
from sklearn.model_selection import train_test_split, GridSearchCV  
from sklearn.metrics import mean_squared_error, r2_score  
import matplotlib.pyplot as plt  
import seaborn as sns  
import chardet  

# File paths  
medal_counts_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_medal_counts.csv'  
athletes_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_athletes.csv'  
programs_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_programs.csv'  

# Function to detect file encoding  
def detect_encoding(file_path):  
    with open(file_path, 'rb') as f:  
        result = chardet.detect(f.read())  
    return result['encoding']  

# Detect encodings  
programs_encoding = detect_encoding(programs_path)  
print(f"Detected encoding for programs file: {programs_encoding}")  

# Load datasets with appropriate encodings  
medal_counts = pd.read_csv(medal_counts_path, encoding='utf-8')  
athletes = pd.read_csv(athletes_path, encoding='utf-8')  
programs = pd.read_csv(programs_path, encoding=programs_encoding)  

# Preprocess medal_counts dataset  
medal_counts['Total_Medals'] = medal_counts[['Gold', 'Silver', 'Bronze']].sum(axis=1)  

# Add GDP and Population placeholders  
np.random.seed(42)  
medal_counts['GDP'] = np.random.uniform(1e9, 1e12, len(medal_counts)).astype(int)  
medal_counts['Population'] = np.random.randint(1e6, 1e8, len(medal_counts))  

# Normalize medal data  
medal_counts['Medals_Per_Million'] = medal_counts['Total_Medals'] / (medal_counts['Population'] / 1e6)
# Step 1: 聚类分析，识别潜力国家  
kmeans = KMeans(n_clusters=4, random_state=42)  
medal_counts['Cluster'] = kmeans.fit_predict(medal_counts[['Total_Medals', 'GDP', 'Population']])  

# 可视化国家的聚类分布  
plt.figure(figsize=(10, 6))  
sns.scatterplot(  
    x='GDP', y='Total_Medals', hue='Cluster', data=medal_counts, palette='viridis', s=100  
)  
plt.title('Clustering Countries Based on Performance', fontsize=14)  
plt.xlabel('GDP', fontsize=12)  
plt.ylabel('Total Medals', fontsize=12)  
plt.legend(title='Cluster')  
plt.tight_layout()  
plt.show()  

# Step 2: 特定体育项目的筛选与推荐  
athletes_grouped = athletes.groupby(['NOC', 'Sport']).agg({  
    'Medal': lambda x: x.notnull().sum()  
}).reset_index()  
athletes_grouped.rename(columns={'Medal': 'Total_Medals'}, inplace=True)  

# Analyze each country's medal distribution by sport  
sports_data = athletes_grouped.groupby('Sport')['Total_Medals'].sum().reset_index()  
sports_data.sort_values(by='Total_Medals', ascending=False, inplace=True)  

# Visualize top Sports  
plt.figure(figsize=(10, 6))  
sns.barplot(x='Total_Medals', y='Sport', data=sports_data.head(10), palette='coolwarm')  
plt.title('Top Sports by Total Medals', fontsize=14)  
plt.xlabel('Total Medals', fontsize=12)  
plt.ylabel('Sport', fontsize=12)  
plt.tight_layout()  
plt.show()  

# Step 3: 教练投资对奖牌数影响的估算  
# Features for random forest regression  
features = ['GDP', 'Population', 'Medals_Per_Million', 'Cluster']  
X = medal_counts[features]  
y = medal_counts['Total_Medals']  

# Train-test split  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  

# Random Forest with hyperparameter tuning  
rf = RandomForestRegressor(random_state=42)  
param_grid = {
    'n_estimators': [100, 200],  
'max_depth': [10, 15],  
'min_samples_split': [2, 5]  
}  
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)  
grid_search.fit(X_train, y_train)  

# Best model  
best_rf = grid_search.best_estimator_  

# Predictions and evaluation  
y_pred = best_rf.predict(X_test)  
mse = mean_squared_error(y_test, y_pred)  
r2 = r2_score(y_test, y_pred)  

print(f"Optimized Random Forest MSE: {mse}")  
print(f"Optimized Random Forest R2: {r2}")  

# Feature importance analysis  
feature_importance = pd.DataFrame({  
    'Feature': features,  
    'Importance': best_rf.feature_importances_  
}).sort_values(by='Importance', ascending=False)  

# 可视化特征重要性  
plt.figure(figsize=(8, 6))  
sns.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis')  
plt.title('Feature Importance in Predicting Total Medals', fontsize=14)  
plt.xlabel('Importance', fontsize=12)  
plt.ylabel('Feature', fontsize=12)  
plt.tight_layout()  
plt.show()  

# Simulating coach investment impact  
medal_counts['Coach_Investment_Effect'] = medal_counts['Total_Medals'] * 1.1  # Assuming 10% boost  

# 可视化教练投资的影响  
plt.figure(figsize=(10, 6))  
sns.scatterplot(  
    x='Total_Medals', y='Coach_Investment_Effect', hue='Cluster',  
    data=medal_counts, palette='coolwarm', s=100  
)  
plt.title('Impact of Coach Investment on Medal Count', fontsize=14)  
plt.xlabel('Original Total Medals', fontsize=12)  
plt.ylabel('Medals After Investment', fontsize=12)  
plt.legend(title='Cluster')  
plt.tight_layout()
plt.show()  

import pandas as pd  
import os  
import numpy as np  
from sklearn.ensemble import RandomForestRegressor  
from sklearn.model_selection import GridSearchCV, train_test_split  
from sklearn.metrics import mean_squared_error, r2_score  
import matplotlib.pyplot as plt  
import seaborn as sns  

# 文件路径  
medal_counts_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_medal_counts.csv'  
athletes_path = 'D:/meisai-2025C/2025_Problem_C_Data/summerOly_athletes.csv'  

# 检查文件路径  
for path in [medal_counts_path, athletes_path]:  
    if not os.path.exists(path):  
        raise FileNotFoundError(f"文件未找到: {path}，请检查路径是否正确。")  

# 加载数据  
medal_counts = pd.read_csv(medal_counts_path, encoding='ISO-8859-1')  
athletes = pd.read_csv(athletes_path, encoding='ISO-8859-1')  

# 模拟 GDP 和人口数据（如果缺失）  
if 'GDP' not in medal_counts.columns:  
    medal_counts['GDP'] = np.random.uniform(1e11, 1e13, size=len(medal_counts))  
if 'Population' not in medal_counts.columns:  
    medal_counts['Population'] = np.random.uniform(1e6, 1e8, size=len(medal_counts))  

# 增加每百万人奖牌数列  
medal_counts['Medals_Per_Million'] = medal_counts['Total'] / (medal_counts['Population'] / 1e6)  

# 筛选潜力国家  
potential_countries = medal_counts[(medal_counts['Total'] < 10) & (medal_counts['GDP'] > 1e12)]  
top_potential_countries = potential_countries.nlargest(3, 'GDP')  
print("推荐的潜力国家:")  
print(top_potential_countries[['NOC', 'Total', 'GDP', 'Population']])  

# 体育项目推荐  
for _, row in top_potential_countries.iterrows():  
    country = row['NOC']  
    print(f"\n分析国家 {country} 的潜力体育项目:")  
    country_data = athletes[athletes['NOC'] == country]
    if country_data.empty:  
    print(f"国家 {country} 没有相关体育项目的历史奖牌数据。推荐高奖牌率体育项目。")  
    top_sports = athletes.groupby('Sport')['Medal'].count().sort_values(ascending=False).head(5)  
    print(f"推荐的全球热门体育项目:\n{top_sports}")  
else:  
    top_sports = country_data.groupby('Sport')['Medal'].count().sort_values(ascending=False).head(5)  
    print(f"国家 {country} 的高奖牌率体育项目:\n{top_sports}")  

# 模型训练与优化  
features = ['GDP', 'Population', 'Medals_Per_Million']  
X = medal_counts[features]  
y = medal_counts['Total']  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  

param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [10, 20, 30]}  
rf = RandomForestRegressor(random_state=42)  
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error')  
grid_search.fit(X_train, y_train)  

# 最优模型  
best_model = grid_search.best_estimator_  
y_pred = best_model.predict(X_test)  

mse = mean_squared_error(y_test, y_pred)  
r2 = r2_score(y_test, y_pred)  
print(f"Optimized Random Forest MSE: {mse}")  
print(f"Optimized Random Forest R2: {r2}")  

# 投资后预测  
print("\n教练投资后潜力国家的奖牌预测:")  
for _, row in top_potential_countries.iterrows():  
    country = row['NOC']  
    country_features = row[features].values.reshape(1, -1)  
    projected_medals = best_model.predict(country_features)[0]  
    print(f"国家 {country}: 原奖牌数 {row['Total']} -> 预测奖牌数 {projected_medals:.2f}")  

# 可视化  
plt.figure(figsize=(10, 6))  
sns.barplot(x=features, y=best_model.feature_importances_, palette='viridis')  
plt.title('Feature Importance for Medal Prediction', fontsize=16)  
plt.xlabel('Feature', fontsize=14)  
plt.ylabel('Importance', fontsize=14)  
plt.tight_layout()  
plt.show()

```

